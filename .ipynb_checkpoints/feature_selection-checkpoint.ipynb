{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use forward selection using AIC criterion on logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "import os.path as path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_out.csv')\n",
    "\n",
    "read = open('train/train_na.pkl', 'rb')\n",
    "train_na = pickle.load(read)\n",
    "read.close()\n",
    "\n",
    "train.index = train.msno\n",
    "train = train[~train_na]\n",
    "train = train.sample(n = 10000)\n",
    "\n",
    "\n",
    "train_X = train.drop(['msno', 'is_churn', 'concated'], axis = 1)\n",
    "train_y = train.is_churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['msno', 'is_churn', '201501', '201502', '201503', '201504', '201505',\n",
       "       '201506', '201507', '201508', '201509', '201510', '201511', '201512',\n",
       "       '201601', '201602', '201603', '201604', '201605', '201606', '201607',\n",
       "       '201608', '201609', '201610', '201611', '201612', '201701', '201702',\n",
       "       'bd', 'registration_init_time', 'expiration_date', 'payment_plan_days',\n",
       "       'transaction_date', 'membership_expire_date', 'is_cancel',\n",
       "       'is_auto_renew', 'num_transactions', 'num_unq', 'total_secs',\n",
       "       'concated', 'consecutive_zeros', 'consecutive_ones', 'city_1.0',\n",
       "       'city_3.0', 'city_4.0', 'city_5.0', 'city_6.0', 'city_7.0', 'city_8.0',\n",
       "       'city_9.0', 'city_10.0', 'city_11.0', 'city_12.0', 'city_13.0',\n",
       "       'city_14.0', 'city_15.0', 'city_16.0', 'city_17.0', 'city_18.0',\n",
       "       'city_19.0', 'city_20.0', 'city_21.0', 'city_22.0', 'city_999.0',\n",
       "       'gender_NA', 'gender_female', 'gender_male', 'registered_via_3.0',\n",
       "       'registered_via_4.0', 'registered_via_7.0', 'registered_via_9.0',\n",
       "       'registered_via_13.0', 'registered_via_999.0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how logistic regression does using all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Guess score:              0.932700\n",
      "Logistic Regression score:     0.968000\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(train_X, train_y)\n",
    "score = log.score(train_X, train_y)\n",
    "print('Best Guess score:              %f' % (1 - np.mean(train_y)))\n",
    "print('Logistic Regression score:     %f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "93% is pretty bad, since that is just the mean of what we are predicting. Thus we must do forward selection to identify relevant variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of forward variable selection is to continually add more variables to our model and seeing which variable creates the most improvements. We will continue iterating only if the model is improving. Otherwise, we will stop in order to avoid taking in noisy and irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "best feature: expiration_date\n",
      "AUC:          0.685736\n",
      "Score:        0.957700\n",
      "Log Loss:     0.199016\n",
      "-----------------------\n",
      "Iteration 2\n",
      "best feature: 201702\n",
      "AUC:          0.751973\n",
      "Score:        0.959100\n",
      "Log Loss:     0.130417\n",
      "-----------------------\n",
      "Iteration 3\n",
      "best feature: is_auto_renew\n",
      "AUC:          0.809379\n",
      "Score:        0.964600\n",
      "Log Loss:     0.096936\n",
      "-----------------------\n",
      "Iteration 4\n",
      "best feature: 201612\n",
      "AUC:          0.823426\n",
      "Score:        0.963800\n",
      "Log Loss:     0.093696\n",
      "-----------------------\n",
      "Iteration 5\n",
      "best feature: payment_plan_days\n",
      "AUC:          0.829997\n",
      "Score:        0.963200\n",
      "Log Loss:     0.090591\n",
      "-----------------------\n",
      "Iteration 6\n",
      "best feature: 201509\n",
      "AUC:          0.831859\n",
      "Score:        0.964100\n",
      "Log Loss:     0.090419\n",
      "-----------------------\n",
      "Iteration 7\n",
      "best feature: city_12.0\n",
      "AUC:          0.833344\n",
      "Score:        0.964300\n",
      "Log Loss:     0.090377\n",
      "-----------------------\n",
      "Iteration 8\n",
      "best feature: city_6.0\n",
      "AUC:          0.833505\n",
      "Score:        0.964600\n",
      "Log Loss:     0.090364\n",
      "-----------------------\n",
      "Iteration 9\n",
      "best feature: gender_male\n",
      "AUC:          0.834034\n",
      "Score:        0.964300\n",
      "Log Loss:     0.090315\n",
      "-----------------------\n",
      "Iteration 10\n",
      "best feature: 201606\n",
      "AUC:          0.835045\n",
      "Score:        0.964900\n",
      "Log Loss:     0.090269\n",
      "-----------------------\n",
      "Iteration 11\n",
      "best feature: 201510\n",
      "AUC:          0.835841\n",
      "Score:        0.965100\n",
      "Log Loss:     0.090264\n",
      "-----------------------\n",
      "Iteration 12\n",
      "Logfit is not improving...\n"
     ]
    }
   ],
   "source": [
    "isImproving = True\n",
    "features = train_X.columns\n",
    "ranked_features = []\n",
    "score = []\n",
    "logloss = []\n",
    "AUC = [0]\n",
    "i = 0\n",
    "while isImproving:\n",
    "    top_AUC = AUC[i]\n",
    "    print('Iteration %s' % str(i+1))\n",
    "    for var in train_X.columns.difference(ranked_features):\n",
    "        log = LogisticRegression()\n",
    "        log.fit(train_X[[var] + ranked_features], train_y)\n",
    "        pred = log.predict(train_X[[var] + ranked_features])\n",
    "        prob = log.predict_proba(train_X[[var] + ranked_features])\n",
    "        curr_AUC = roc_auc_score(train_y, pred)\n",
    "        if curr_AUC > top_AUC:\n",
    "            best_feature = var\n",
    "            top_AUC = curr_AUC\n",
    "            best_score= np.mean(pred == train_y)\n",
    "            top_log_loss = log_loss(train_y, prob)\n",
    "    if(AUC[i] >= top_AUC):\n",
    "        print('Logfit is not improving...')\n",
    "        isImproving = False\n",
    "        break\n",
    "    else:\n",
    "        print('best feature: %s' % best_feature)\n",
    "        print('AUC:          %f' % top_AUC)\n",
    "        print('Score:        %f' % best_score)\n",
    "        print('Log Loss:     %f' % top_log_loss)\n",
    "        print('-----------------------')\n",
    "        AUC.append(top_AUC)\n",
    "        ranked_features.append(best_feature)\n",
    "        score.append(best_score)\n",
    "        logloss.append(top_log_loss)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranked_features = ['expiration_date',\n",
    "                  'membership_expire_date',\n",
    "                  'registered_via_7.0',\n",
    "                  'is_auto_renew',\n",
    "                  'is_cancel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Variable Selection with Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the best features with cross validation fold (n_fold = 5). This will give a better idea ofwhat our actual score will be in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fold = 3\n",
    "cv = StratifiedKFold(n_splits = n_fold, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.reset_index().drop(['msno'], axis = 1)\n",
    "train_y = train_y.reset_index().drop(['msno'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-0b009f580341>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mcurr_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi_trn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi_val\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michael\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    320\u001b[0m                                                              n_samples))\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_BaseKFold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michael\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter_test_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m             \u001b[0mtrain_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0mtest_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michael\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_iter_test_masks\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_iter_test_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m         \u001b[0mtest_folds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_folds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mtest_folds\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michael\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_make_test_folds\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtest_fold_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mper_cls_splits\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mper_cls_cvs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_split\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mper_cls_splits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m                 \u001b[0mcls_test_folds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_folds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m                 \u001b[1;31m# the test split can be too big because we used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                 \u001b[1;31m# KFold(...).split(X[:max(c, n_splits)]) when data is not 100%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "isImproving = True\n",
    "features = train_X.columns\n",
    "ranked_features2 = []\n",
    "score = []\n",
    "curr_score = []\n",
    "logloss = []\n",
    "curr_logloss = []\n",
    "AUC = [[0]]\n",
    "curr_cv_AUC = []\n",
    "i = 0\n",
    "while isImproving:\n",
    "    top_AUC = np.mean(AUC[i])\n",
    "    print('Iteration %s' % str(i+1))\n",
    "    for var in train_X.columns.difference(ranked_features2):\n",
    "        ### Add CV\n",
    "        \n",
    "        curr_cv_AUC = []\n",
    "        curr_logloss = []\n",
    "        curr_score = []\n",
    "        \n",
    "        for i_trn, i_val in cv.split(train_X, train_y):\n",
    "            \n",
    "            log = LogisticRegression()\n",
    "            log.fit(train_X.loc[i_trn, [var] + ranked_features2], train_y[i_trn])\n",
    "            pred = log.predict(train_X.loc[i_val, [var] + ranked_features2])\n",
    "            prob = log.predict_proba(train_X.loc[i_val, [var] + ranked_features2])\n",
    "            curr_AUC = roc_auc_score(train_y[i_val], pred)\n",
    "            curr_cv_AUC.append(curr_AUC)\n",
    "            curr_score.append(pred == train_y[i_val])\n",
    "            curr_logloss.append(log_loss(train_y[i_val], prob))\n",
    "          \n",
    "        if np.mean(curr_cv_AUC) > np.mean(top_AUC):\n",
    "            best_feature = var\n",
    "            top_AUC = curr_cv_AUC\n",
    "            best_score= curr_score\n",
    "            top_log_loss = curr_logloss\n",
    "\n",
    "    if(np.mean(AUC[i]) >= np.mean(top_AUC)):\n",
    "        print('Logfit is not improving...')\n",
    "        isImproving = False\n",
    "        break\n",
    "    else:\n",
    "        print('best feature: %s' % best_feature)\n",
    "        print('AUC:          %f' % np.mean(top_AUC))\n",
    "        print('Score:        %f' % np.mean(best_score))\n",
    "        print('Log Loss:     %f' % np.mean(top_log_loss))\n",
    "        print('-----------------------')\n",
    "        score.append(curr_score)\n",
    "        logloss.append(curr_logloss)\n",
    "        AUC.append(curr_cv_AUC)\n",
    "        AUC.append(top_AUC)\n",
    "        ranked_features2.append(best_feature)\n",
    "        score.append(best_score)\n",
    "        logloss.append(top_log_loss)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.22852491985673834, 0.22771551762172304, 0.22826281756063532],\n",
       " [0.19168042715323549, 0.19166939403941957, 0.19118863098225081],\n",
       " [0.18128228601947433, 0.18068370876893211, 0.18071214246597558],\n",
       " [0.14575567569476386, 0.14517778847512317, 0.14490048127113253],\n",
       " [0.14165583797073164, 0.14080240696779375, 0.14086321786718495],\n",
       " [0.13762068423989871, 0.13649567387260006, 0.13640331458236057],\n",
       " [0.13596901720417562, 0.13467281080735002, 0.13479734456770029],\n",
       " [0.12979934839749135, 0.1289415959384472, 0.1290855058112341],\n",
       " [0.12818631881663184, 0.12716833201299571, 0.12756544221999314],\n",
       " [0.12818631881663184, 0.12716833201299571, 0.12756544221999314]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss of Logistic Regression:         0.094033\n",
      "Accuracy of Logistic Regression:         0.986200\n",
      "Logistic Regression Probs Saved ~~~~\n",
      "Best features saved!\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(train_X[ranked_features], train_y)\n",
    "prob = log.predict_proba(train_X[ranked_features])\n",
    "predict = log.predict(train_X[ranked_features])\n",
    "print(\"Log Loss of Logistic Regression:         %f\" % log_loss(train_y, prob))\n",
    "print('Accuracy of Logistic Regression:         %f' % (1 - np.mean(train_y - predict)))\n",
    "output = open('models/log_probs.pkl', 'wb')\n",
    "pickle.dump(prob, output, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "output.close\n",
    "print('Logistic Regression Probs Saved ~~~~')\n",
    "output = open('train/best_features.pkl', 'wb')\n",
    "pickle.dump(ranked_features, output, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "output.close()\n",
    "print('Best features saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
