{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, relevant features will be extracted using stepwise logistic regression. The basic idea is to fit the model with one additional predictor, only choosing the variable that lowers AUC the most, and continually iterating until AUC does not improve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "import os.path as path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_out.csv')\n",
    "\n",
    "read = open('train/train_na.pkl', 'rb')\n",
    "train_na = pickle.load(read)\n",
    "read.close()\n",
    "\n",
    "train.index = train.msno\n",
    "# I'm only taking training datapoints without NA's\n",
    "train = train[~train_na]\n",
    "# to make things faster, I'm only using 10% of the data\n",
    "train = train.sample(n = 100000) \n",
    "\n",
    "train_X = train.drop(['msno', 'is_churn', 'concated'], axis = 1)\n",
    "train_y = train.is_churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will do Logistic Regression using all the features to get a sense of how much improvement we will make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guessing the mode score:           0.934550\n",
      "Logistic Regression score:         0.934550\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(train_X, train_y)\n",
    "score = log.score(train_X, train_y)\n",
    "print('Guessing the mode score:           %f' % (1 - np.mean(train_y)))\n",
    "print('Logistic Regression score:         %f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score is actually bad because it's exactly equivalent to guessing all 0's. This is why we must subset the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepwise Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of forward variable selection is to continually add more variables to our model and seeing which variable creates the most improvements. We will continue iterating only if the model is improving. Otherwise, we will stop in order to avoid taking in noisy and irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Best feature:      expiration_date\n",
      "AUC:               0.709457\n",
      "Score:             0.961010\n",
      "Log Loss:          0.188780\n",
      "Iteration 2\n",
      "Best feature:      201702\n",
      "AUC:               0.756242\n",
      "Score:             0.960950\n",
      "Log Loss:          0.128497\n",
      "Iteration 3\n",
      "Best feature:      is_auto_renew\n",
      "AUC:               0.807173\n",
      "Score:             0.965980\n",
      "Log Loss:          0.098397\n",
      "Iteration 4\n",
      "Best feature:      membership_expire_date\n",
      "AUC:               0.826610\n",
      "Score:             0.966590\n",
      "Log Loss:          0.095237\n",
      "Iteration 5\n",
      "Best feature:      201503\n",
      "AUC:               0.829784\n",
      "Score:             0.966150\n",
      "Log Loss:          0.094140\n",
      "Iteration 6\n",
      "Best feature:      registered_via_7.0\n",
      "AUC:               0.832299\n",
      "Score:             0.966600\n",
      "Log Loss:          0.093630\n",
      "Iteration 7\n",
      "Best feature:      is_cancel\n",
      "AUC:               0.834409\n",
      "Score:             0.966560\n",
      "Log Loss:          0.091545\n",
      "Iteration 8\n",
      "Best feature:      201609\n",
      "AUC:               0.835422\n",
      "Score:             0.966860\n",
      "Log Loss:          0.091444\n",
      "Iteration 9\n",
      "Best feature:      num_transactions\n",
      "AUC:               0.836321\n",
      "Score:             0.966550\n",
      "Log Loss:          0.091270\n",
      "Iteration 10\n",
      "Best feature:      201611\n",
      "AUC:               0.837963\n",
      "Score:             0.966830\n",
      "Log Loss:          0.090535\n",
      "Iteration 11\n",
      "Best feature:      201607\n",
      "AUC:               0.838285\n",
      "Score:             0.966900\n",
      "Log Loss:          0.090526\n",
      "Iteration 12\n",
      "Best feature:      city_10.0\n",
      "AUC:               0.838437\n",
      "Score:             0.966920\n",
      "Log Loss:          0.090514\n",
      "Iteration 13\n",
      "Best feature:      201505\n",
      "AUC:               0.838448\n",
      "Score:             0.966940\n",
      "Log Loss:          0.090513\n",
      "Iteration 14\n",
      "Best feature:      201506\n",
      "AUC:               0.838677\n",
      "Score:             0.966970\n",
      "Log Loss:          0.090500\n",
      "Iteration 15\n",
      "Best feature:      num_unq\n",
      "AUC:               0.838868\n",
      "Score:             0.967060\n",
      "Log Loss:          0.090478\n",
      "Iteration 16\n",
      "Best feature:      registered_via_13.0\n",
      "AUC:               0.839102\n",
      "Score:             0.967100\n",
      "Log Loss:          0.090416\n",
      "Iteration 17\n",
      "Best feature:      city_3.0\n",
      "AUC:               0.839260\n",
      "Score:             0.967130\n",
      "Log Loss:          0.090415\n",
      "Iteration 18\n",
      "Logfit is not improving...\n"
     ]
    }
   ],
   "source": [
    "isImproving = True\n",
    "features = train_X.columns\n",
    "ranked_features = []\n",
    "score = []\n",
    "logloss = []\n",
    "AUC = [0]\n",
    "i = 0\n",
    "\n",
    "while isImproving:\n",
    "    top_AUC = AUC[i]\n",
    "    print('Iteration %s' % str(i+1))\n",
    "    for var in train_X.columns.difference(ranked_features):\n",
    "        log = LogisticRegression()\n",
    "        log.fit(train_X[[var] + ranked_features], train_y)\n",
    "        pred = log.predict(train_X[[var] + ranked_features])\n",
    "        prob = log.predict_proba(train_X[[var] + ranked_features])\n",
    "        curr_AUC = roc_auc_score(train_y, pred)\n",
    "        if curr_AUC > top_AUC:\n",
    "            best_feature = var\n",
    "            top_AUC = curr_AUC\n",
    "            best_score = np.mean(pred == train_y)\n",
    "            top_log_loss = log_loss(train_y, prob)\n",
    "    if(AUC[i] >= top_AUC):\n",
    "        print('Logfit is not improving...')\n",
    "        isImproving = False\n",
    "    else:\n",
    "        print('Best feature:      %s' % best_feature)\n",
    "        print('AUC:               %f' % top_AUC)\n",
    "        print('Score:             %f' % best_score)\n",
    "        print('Log Loss:          %f' % top_log_loss)\n",
    "        AUC.append(top_AUC)\n",
    "        ranked_features.append(best_feature)\n",
    "        score.append(best_score)\n",
    "        logloss.append(top_log_loss)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how much we improved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss of Logistic Regression:        0.090415\n",
      "Accuracy of Logistic Regression:        0.992570\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(train_X[ranked_features], train_y)\n",
    "prob = log.predict_proba(train_X[ranked_features])\n",
    "predict = log.predict(train_X[ranked_features])\n",
    "print('Log Loss of Logistic Regression:        %f' % log_loss(train_y, prob))\n",
    "print('Accuracy of Logistic Regression:        %f' % (1 - np.mean(train_y - predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now logistic regression is performing 99% accurate on the training data. Of course, accuracy will be very different in the test data, and we will use other models (Random forest, gradient boosting, etc) to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's save the best features for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best features saved!\n"
     ]
    }
   ],
   "source": [
    "output = open('train/best_features.pkl', 'wb')\n",
    "pickle.dump(ranked_features, output, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "output.close()\n",
    "print('Best features saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
