{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import os.path as path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ef56495b4a8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'201702'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'201702'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mtext_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mtest_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Michael\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, y, copy)\u001b[0m\n\u001b[0;32m    644\u001b[0m         X = check_array(X, accept_sparse='csr', copy=copy,\n\u001b[0;32m    645\u001b[0m                         \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michael\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michael\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 58\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "#param_grid = {'hidden_layer_sizes': [(5, ), (10, ), (5,3), (10, 5)], 'momentum': [0.5, 0.7, 0.9]}\n",
    "\n",
    "save_dir = path.join(path.dirname(os.getcwd()), 'models')\n",
    "data_dir = path.join(path.dirname(os.getcwd()), 'data', 'train_out.csv')\n",
    "\n",
    "read = open('best_features.pkl', 'rb')\n",
    "ranked_features = pickle.load(read)\n",
    "read.close()\n",
    "\n",
    "read = open('train_na.pkl', 'rb')\n",
    "train_na = pickle.load(read)\n",
    "read.close()\n",
    "\n",
    "read = open('test_na.pkl', 'rb')\n",
    "test_na = pickle.load(read)\n",
    "read.close()\n",
    "\n",
    "train_out = pd.read_csv(data_dir)\n",
    "train_out.index = train_out.msno\n",
    "avg = np.mean(train_out.loc[train_na, 'is_churn'])\n",
    "train_out = train_out[~train_na]\n",
    "train_X = train_out.drop(['msno', 'concated', 'is_churn'], axis = 1)\n",
    "#train_X = train_X[ranked_features]\n",
    "train_y = train_out.is_churn\n",
    "\n",
    "data_dir = path.join(path.dirname(os.getcwd()), 'data', 'test_out.csv')\n",
    "test_out = pd.read_csv(data_dir)\n",
    "test_out.index = test_out.msno\n",
    "test_X = test_out.drop(['msno', 'concated', 'is_churn'], axis = 1)\n",
    "#test_X = test_X[ranked_features]\n",
    "test_y = test_out.is_churn\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_X)\n",
    "train_X = pd.DataFrame(scaler.transform(train_X))\n",
    "test_X.loc[test_X['201702'].isnull(), '201702'] = 0\n",
    "text_X = test_X.fillna(0)\n",
    "test_X = pd.DataFrame(scaler.transform(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'consecutive_ones'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c711aec8c628>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsecutive_ones\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Michael\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2670\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2671\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2672\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'consecutive_ones'"
     ]
    }
   ],
   "source": [
    "train_X.consecutive_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prev_param = False\n",
    "param_grid = {'hidden_layer_sizes': [(30,5)], 'momentum': [0.7, 0.9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Searching Parameters...\n",
      "-----------------------------\n",
      "Preparing Grid Search...\n",
      "------------------------------\n",
      "Training on the data.... \n"
     ]
    }
   ],
   "source": [
    "if prev_param:\n",
    "    print('Training on saved best parameters...')\n",
    "    read = open('best_param.pkl', 'rb')\n",
    "    best_param = pickle.load(read)\n",
    "    read.close()\n",
    "    print('Finished Reading best_param...')\n",
    "    print('Setting up nnet')\n",
    "    nnet = MLPClassifier(solver = 'sgd', max_iter = 500, random_state = 1, verbose = 1, **best_param)\n",
    "    print('Training on the data...')\n",
    "    nnet.fit(train_X, train_y)\n",
    "    print(nnet)\n",
    "    print('Score: %f' % nnet.score(train_X, train_y))\n",
    "else:\n",
    "    print('Grid Searching Parameters...')\n",
    "    print('-----------------------------')\n",
    "    print('Preparing Grid Search...')\n",
    "    print('------------------------------')\n",
    "    nnet = MLPClassifier(solver = 'sgd', random_state = 1, learning_rate_init = 1e-4)\n",
    "    clf = GridSearchCV(nnet, param_grid, cv = 5)\n",
    "    print('Training on the data.... ')\n",
    "    clf.fit(train_X, train_y)\n",
    "    print('------------------------------')\n",
    "    print('It complete! (* ^ ω ^)')\n",
    "    print('------------------------------')\n",
    "    print(clf.cv_results_)\n",
    "    print('Best Parameter')\n",
    "    print(clf.best_params_)\n",
    "    print('Best Score')\n",
    "    print(clf.best_score_)\n",
    "    output = open('best_param.pkl', 'wb')\n",
    "    pickle.dump(clf.best_params_, output, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.35451394\n",
      "Iteration 2, loss = 0.11906003\n",
      "Iteration 3, loss = 0.10679501\n",
      "Iteration 4, loss = 0.13733716\n",
      "Iteration 5, loss = 0.12675455\n",
      "Iteration 6, loss = 0.10041579\n",
      "Iteration 7, loss = 0.09757759\n",
      "Iteration 8, loss = 0.09427815\n",
      "Iteration 9, loss = 0.13202235\n",
      "Iteration 10, loss = 0.35984181\n",
      "Iteration 11, loss = 0.23978822\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "nnet = MLPClassifier(verbose = 1, hidden_layer_sizes = (50,30,10), \n",
    "                     learning_rate_init=0.0005, max_iter = 500, random_state = 1, momentum = 0.9)\n",
    "nnet.fit(train_X, train_y)\n",
    "prob = nnet.predict_proba(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnet log loss:           0.080982\n",
      "NNet probs saved!\n",
      "NNet Saved!\n"
     ]
    }
   ],
   "source": [
    "print('nnet log loss:           %f' % log_loss(train_y, prob))\n",
    "save_dir = path.join(path.dirname(os.getcwd()), 'models', 'nnet_probs.pkl')\n",
    "output = open(save_dir, 'wb')\n",
    "pickle.dump(prob, output, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "output.close()\n",
    "print('NNet probs saved!')\n",
    "save_dir = path.join(path.dirname(os.getcwd()), 'models', 'nnet.pkl')\n",
    "output = open(save_dir, 'wb')\n",
    "pickle.dump(nnet, output, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "output.close()\n",
    "print('NNet Saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Nnet Predictions to:      C:\\Users\\Michael\\Documents\\python\\kkbox\\submissions\\gbc_submission.csv\n"
     ]
    }
   ],
   "source": [
    "test_X = test_X.loc[~test_na]\n",
    "\n",
    "pred = nnet.predict_proba(test_X)\n",
    "pred = pred[:,1]\n",
    "index = test_out['msno']\n",
    "d = {\n",
    "    'msno': index,\n",
    "    'is_churn': pred\n",
    "}\n",
    "\n",
    "nnet_submission = pd.DataFrame(d)\n",
    "\n",
    "nnet_submission = nnet_submission.append(pd.DataFrame({'msno': test_na[test_na == True].index, \n",
    "                                    'is_churn': [avg] * len(test_na[test_na == True].index)}))\n",
    "\n",
    "nnet_submission.index = nnet_submission.msno\n",
    "\n",
    "\n",
    "save_dir = path.join(path.dirname(os.getcwd()), 'submissions')\n",
    "nnet_submission.to_csv(path.join(save_dir, 'nnet_submission.csv'), index = False)\n",
    "print('Saved Nnet Predictions to:      %s' % path.join(save_dir, 'gbc_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043923835187549894"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(nnet_submission.is_churn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
